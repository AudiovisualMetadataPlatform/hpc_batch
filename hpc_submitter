#!/bin/env python3
import argparse
import paramiko
import getpass
from pathlib import Path
import logging
import sys
import os
import socket
import time
import stat
import configparser
import tempfile
import yaml
import re
import traceback

def main():
    """
    Manage the job dropbox, submit jobs to the dropbox on HPC, and submit the
    hpc_runner to the HPC processing queue (if needed)
    """
    parser = argparse.ArgumentParser(description=main.__doc__)
    parser.add_argument("--debug", default=False, action="store_true", help="Turn on debugging")
    parser.add_argument("--nocleanup", default=False, action="store_true", help="Don't clean up the remote directories")
    parser.add_argument("--email", default=None, type=str, help="Email address for status reports")
    parser.add_argument("config", help="Configuration file")
    args = parser.parse_args()

    logging.basicConfig(level=logging.DEBUG if args.debug else logging.INFO, 
                        stream=sys.stderr,
                        format="%(asctime)s %(levelname)s %(message)s")

    # read the configuration file
    config = configparser.ConfigParser()
    config.read(args.config)

    lockfile = Path(config['cronjob']['lockfile'])
    if lockfile.exists():
        exit(0)

    try:
        # create the lock
        with open(lockfile, mode="w") as f:
            f.write(str(os.getpid()) + "\n")

        # connect to the remote machine and go to our workspace directory
        with paramiko.SSHClient() as ssh:
            ssh.set_missing_host_key_policy(paramiko.AutoAddPolicy())
            ssh.connect(hostname=config['remote']['host'], 
                        port=config['remote']['port'], 
                        username=config['remote']['user'],
                        key_filename=config['remote']['key_file'],
                        passphrase=config['remote']['passphrase'])

            try:
                sftp = ssh.open_sftp()
                sftp.chdir(config['hpc_env']['workspace'])
            except Exception as e:
                raise Exception(f"Cannot change to base directory {config['hpc_env']['workspace']}: {e}")
                
            # Push any new jobs to the HPC dropbox
            dropbox = Path(config['cronjob']['dropbox'])
            for jobfile in dropbox.glob("*.job"):
                try:
                    send_job(ssh, jobfile, config['hpc_env'])
                except Exception as e:
                    logging.error(f"Cannot submit {jobfile}: {e}")

            # Submit the remote runner, if it isn't already
            submit_runner(ssh, 
                          email=args.email,
                          memory=config['hpc']['memory'],
                          partition=config['hpc']['partition'],
                          resource=config['hpc']['gres'])

            # capture any ready output
            get_results(ssh, nocleanup=args.nocleanup)
    
    except Exception as e:        
        logging.error(e)
        traceback.print_exc()
    finally:
        lockfile.unlink()


def send_job(ssh, jobfile, hpc_env):
    """
    Copy the files to the HPC cluster
    """
    sftp = ssh.open_sftp()
    with open(jobfile) as f:
        job = yaml.load(f, Loader=yaml.SafeLoader)


    # Create the workspace directory by using the submitter's hostname, 
    # the inode of the submitted job file and the current time.
    workdir = f"/{socket.gethostname().split('.')[0]}-{jobfile.stat().st_ino}-{time.time()}"
    workspace = hpc_env['workspace'] + workdir
    job['hpc_workspace'] = workspace
    job['finished_file'] = f"{workspace}/finished.out"

    logging.debug(f"Creating work directory: {workspace}")
    sftp.mkdir(workspace)
    sftp.chdir(workspace)

    # copy the args into the params
    params = {}
    params.update(hpc_env)
    if 'args' in job:
        params.update(job['args'])

    # copy the input files to the workspace    
    sftp.mkdir('input')
    sftp.chdir('input')
    job['input_map_hpc'] = {}
    for ifile, localfile in job['input_map'].items():
        localfile = Path(localfile)
        remotefile = workspace + "/input/" + localfile.name
        job['input_map_hpc'][ifile] = remotefile
        logging.debug(f"Copying {localfile} to {remotefile}")
        sftp.put(localfile, remotefile)
        params[ifile] = localfile.name
    sftp.chdir("..")
    
    # create the map for the output files
    sftp.mkdir('output')
    job['output_map_hpc'] = {}
    for ofile, localfile in job['output_map'].items():
        localfile = Path(localfile)
        remotefile = workspace + "/output/" + localfile.name
        job['output_map_hpc'][ofile] = remotefile
        params[ofile] = localfile.name

    # build the job script 
    with tempfile.NamedTemporaryFile(mode="w", delete=False) as f:
        hpc_jobfile = f.name
        # Insert the run script here
        template = Path(sys.path[0], f"tools/{job['script']}.template")
        with open(template) as t:
            for l in t.readlines():
                while m := re.search(r"<<([a-zA-Z_][a-zA-Z0-9_]*?)>>", l):
                    key = m.group(1)
                    if key in params:
                        l = re.sub(f"<<{key}>>", params[key], l)
                    else:
                        logging.warning(f"Key '{key}' desired by script template, but not set in job")
                        break
                f.write(l)
        

    sftp.put(hpc_jobfile, workspace + "/job.sh")
    sftp.chmod(workspace + "/job.sh", 0o750)

    # write the updated job file as .submitted
    with open(str(jobfile) + ".submitted", "w") as f:
        yaml.dump(job, f)
    # remove the original job file
    jobfile.unlink()
    sftp.close()


def submit_runner(ssh, email=None,
             memory=32, partition=None, resource=None):
    """
    Submit the hpc_runner program to the slurm queue if it isn't already
    running or already in the queue
    """
    workspace="/foo"  # TODO
    with tempfile.NamedTemporaryFile(mode="w", delete=False) as f:
        hpc_jobfile = f.name
        f.write("#!/bin/bash\n")
        f.write(f"#SBATCH -p {partition}\n")
        f.write(f"#SBATCH --gres {resource}\n")
        f.write(f"#SBATCH -o {workspace}/stdout.txt\n")
        f.write(f"#SBATCH -e {workspace}/stderr.txt\n")
        if email is not None:
            f.write("#SBATCH --mail-type=ALL\n")
            f.write(f"#SBATCH --mail-user={email}\n")
        f.write(f"#SBATCH --mem={memory}G\n")
    

    pass

def get_results(ssh, nocleanup=False):
    """
    Check for completed jobs, copy the output files, create the results file,
    and clean up the HPC side of things.
    """
    sftp = ssh.open_sftp()
    pass








def old():
    """
            # now that the job exists on carbonate, queue it
            command = f"cd {workspace}; sbatch {workspace}/job.sh; echo $?"
            logging.info(f"Running: {command}")
            _, stdout, stderr = ssh.exec_command(command)
            stdout = stdout.readlines()
            rc = int(stdout[-1])
            logging.info(f"Return code: {rc}")
            logging.debug(f"STDOUT: {''.join(stdout)}")
            logging.debug(f"STDERR: {''.join(stderr.readlines())}")

            if rc != 0:
                raise Exception(f"Cannot queue job RC={rc}")

            # the job is queued, so we just need to wait for the finished.out
            # file to appear (which contains the return code)
            poll = 30
            logging.info(f"Checking every {poll}s for {workspace}/finished.out to appear")
            while True:
                try:
                    sftp.stat(f"{workspace}/finished.out")
                    logging.info(f"{workspace}/finished.out has appeared")
                    break
                except FileNotFoundError as e:
                    # we're expecting this.
                    logging.debug(f"Still waiting.")
                    time.sleep(poll)

            with tempfile.TemporaryDirectory() as td:
                logging.debug("Retrieving finished.out")
                sftp.get(f"{workspace}/finished.out", f"{td}/finished.out")
                with open(f"{td}/finished.out") as f:
                    rc = int(f.readline())
                if rc != 0:
                    # uh oh.  Grab the stdout and stderr bits and dump them.
                    logging.error(f"Not-zero return code from batch job: {rc}")
                    for out in ("stdout.txt", "stderr.txt"):
                        try:
                            sftp.get(f"{workspace}/{out}", f"{td}/{out}")
                            with open(f"{td}/{out}") as f:
                                output = f.read()
                            logging.error(f"Contents of {out}:\n{output}")
                        except Exception as e:
                            pass
                    error = True
                else:
                    for o in out_map:
                        logging.info(f"Retrieving {o} as {out_map[o]}")
                        sftp.get(f"{workspace}/{o}", out_map[o])
                        
        except Exception as e:
            logging.critical(f"Error during processing: {e}")
            error = True
        finally:
            if args.nocleanup:
                logging.info(f"Not cleaning up remote directory {workspace}")
            else:
                logging.info(f"Cleaning up work directory {workspace}")
                sftp.chdir("..")
                clean_up(sftp, workdir)

    exit(1 if error else rc)
"""


def clean_up(sftp, dir):
    entries = list(sftp.listdir_iter(dir))
    for f in entries:
        if f.st_mode & stat.S_IFDIR:
            clean_up(sftp, f"{dir}/{f.filename}")
        else:
            logging.debug(f"Removing {dir}/{f.filename}")
            sftp.remove(f"{dir}/{f.filename}")
    logging.debug(f"Removing directory {dir}")
    sftp.rmdir(dir)

if __name__ == "__main__":
    main()